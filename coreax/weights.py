# Â© Crown Copyright GCHQ
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

r"""
Classes and associated functionality to optimise weighted representations of data.

Several aspects of this codebase take a :math:`n \times d` dataset and generate an
alternative representation of it, for example a coreset. The quality of this alternative
representation in approximating the original dataset can be assessed using some metric
of interest, for example see :class:`~coreax.metrics.Metric`.

One can improve the quality of the representation generated by weighting the individual
elements of it. These weights are determined by optimising the metric of interest, which
compares the original :math:`n \times d` dataset and the generated representation of it.

This module provides functionality to calculate such weights, through various methods.
All methods implement :class:`WeightsOptimiser` and must have a
:meth:`~WeightsOptimiser.solve` method that, given two datasets, returns an array of
weights such that a metric of interest is optimised when these weights are applied to
the dataset.
"""

from abc import abstractmethod
from typing import Generic, TypeVar, Union

import equinox as eqx
import jax.numpy as jnp
from jax import Array
from jax.typing import ArrayLike
from typing_extensions import deprecated

from coreax.data import Data, SupervisedData, as_data, as_supervised_data
from coreax.kernel import Kernel, TensorProductKernel
from coreax.util import solve_qp

_Data = TypeVar("_Data", bound=Data)
_SupervisedData = TypeVar("_SupervisedData", bound=SupervisedData)


def _prepare_kernel_system(
    kernel: Union[Kernel, TensorProductKernel],
    x: Union[Data, SupervisedData],
    y: Union[Data, SupervisedData],
    epsilon: float = 1e-10,
    *,
    block_size: Union[int, None, tuple[Union[int, None], Union[int, None]]] = None,
    unroll: Union[int, bool, tuple[Union[int, bool], Union[int, bool]]] = 1,
) -> tuple[Array, Array]:
    r"""
    Return the row mean of :math`k(y, x)` and the Gramian :math:`k(y, y)`.

    :param kernel: :class:`~coreax.kernel.Kernel` instance implementing a kernel
        function :math:`k: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}` if
        working with unsupervised :class:`~coreax.data.Data`, else
        :class:`~coreax.kernel.TensorProductKernel` instance if working with
        :class:`~coreax.data.SupervisedData`
    :param x: :class:`~coreax.data.Data` instance consisting of a :math:`n \times d`
        data array or :class:`~coreax.data.SupervisedData` instance consisting of
        :math:`n \times d` data array paired with :math:`n \times p` supervision array
    :param y: :class:`~coreax.data.Data` instance consisting of a :math:`m \times d`
        data array or :class:`~coreax.data.SupervisedData` instance consisting of
        :math:`m \times d` data array paired with :math:`m \times p` supervision array,
        representing a coreset
    :param epsilon: Small positive value to add to the kernel Gram matrix to aid
        numerical solver computations
    :param block_size: Block size passed to the ``self.kernel.compute_mean``
    :param unroll: Unroll parameter passed to ``self.kernel.compute_mean``
    :return: The row mean of k(y,x) and the epsilon perturbed Gramian k(y,y)
    """
    # Check for invalid combinations
    error_msg = (
        "Invalid combination of 'kernel' and 'x' or 'y'; if solving weights for"
        + " 'SupervisedData' or tuple of arrays, one must pass a 'TensorProductKernel',"
        + " if solving weights for 'Data' or an array, one must pass child of 'Kernel'"
    )
    # pylint: disable=unidiomatic-typecheck
    if isinstance(kernel, TensorProductKernel) and (
        (type(x) is not SupervisedData) or (type(y) is not SupervisedData)
    ):
        raise ValueError(error_msg)
    if isinstance(kernel, Kernel) and ((type(x) is not Data) or (type(y) is not Data)):
        raise ValueError(error_msg)
    # pylint: enable=unidiomatic-typecheck
    kernel_yx = kernel.compute_mean(y, x, axis=1, block_size=block_size, unroll=unroll)
    kernel_yy = kernel.compute(y, y) + epsilon * jnp.identity(len(y))
    return kernel_yx, kernel_yy


class WeightsOptimiser(eqx.Module, Generic[_Data]):
    r"""Base class for optimising weights."""

    @abstractmethod
    def solve(self, x: _Data, y: _Data) -> Array:
        r"""
        Calculate the weights.

        :param x: :class:`~coreax.data.Data` instance consisting of a :math:`n \times d`
            data array or :class:`~coreax.data.SupervisedData` instance consisting of
            :math:`n \times d` data array paired with :math:`n \times p` supervision
            array
        :param y: :class:`~coreax.data.Data` instance consisting of a :math:`m \times d`
            data array or :class:`~coreax.data.SupervisedData` instance consisting of
            :math:`m \times d` data array paired with :math:`m \times p` supervision
            array, representing a coreset
        :return: Optimal weighting of points in ``y`` to represent ``x``
        """


class SBQWeightsOptimiser(WeightsOptimiser[_Data]):
    r"""
    Define the Sequential Bayesian Quadrature (SBQ) optimiser class.

    References for this technique can be found in :cite:`huszar2016optimally`.
    Weights determined by SBQ are equivalent to the unconstrained weighted maximum mean
    discrepancy (MMD) optimum.

    The Bayesian quadrature estimate of the integral

    .. math::

        \int f(x) p(x) dx

    can be viewed as a  weighted version of kernel herding. The Bayesian quadrature
    weights, :math:`w_{BQ}`, are given by

    .. math::

        w_{BQ}^{(n)} = \sum_m z_m^T K_{mn}^{-1}

    for a dataset :math:`x` with :math:`n` points, and coreset :math:`y` of :math:`m`
    points. Here, for given kernel :math:`k`, we have :math:`z = \int k(x, y)p(x) dx`
    and :math:`K = k(y, y)` in the above expression. See equation 20 in
    :cite:`huszar2016optimally` for further detail.

    :param kernel: :class:`~coreax.kernel.Kernel` instance implementing a kernel
        function :math:`k: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}`
    """

    kernel: Kernel

    def solve(
        self,
        x: Union[ArrayLike, Data],
        y: Union[ArrayLike, Data],
        epsilon: float = 1e-10,
        *,
        block_size: Union[int, None, tuple[Union[int, None], Union[int, None]]] = None,
        unroll: Union[int, bool, tuple[Union[int, bool], Union[int, bool]]] = 1,
        **solver_kwargs,
    ) -> Array:
        r"""
        Calculate weights from Sequential Bayesian Quadrature (SBQ).

        References for this technique can be found in
        :cite:`huszar2016optimally`. These are equivalent to the unconstrained
        weighted maximum mean discrepancy (MMD) optimum.

        Note that weights determined through SBQ do not need to sum to 1, and can be
        negative.

        :param x: :class:`~coreax.data.Data` instance consisting of a :math:`n \times d`
            data array
        :param y: :class:`~coreax.data.Data` instance consisting of a :math:`m \times d`
            data array, representing a coreset
        :param epsilon: Small positive value to add to the kernel Gram matrix to aid
            numerical solver computations
        :param block_size: Block size passed to the ``self.kernel.compute_mean``
        :param unroll: Unroll parameter passed to ``self.kernel.compute_mean``
        :param solver_kwargs: Additional kwargs passed to ``jnp.linalg.solve``
        :return: Optimal weighting of points in ``y`` to represent ``x``
        """
        kernel_yx, kernel_yy = _prepare_kernel_system(
            self.kernel,
            as_data(x),
            as_data(y),
            epsilon,
            block_size=block_size,
            unroll=unroll,
        )
        return jnp.linalg.solve(kernel_yy, kernel_yx, **solver_kwargs)


class MMDWeightsOptimiser(WeightsOptimiser[_Data]):
    r"""
    Define the MMD weights optimiser class.

    This optimiser solves a simplex weight problem of the form:

    .. math::

        \mathbf{w}^{\mathrm{T}} \mathbf{k} \mathbf{w} +
        \bar{\mathbf{k}}^{\mathrm{T}} \mathbf{w} = 0

    subject to

    .. math::

        \mathbf{Aw} = \mathbf{1}, \qquad \mathbf{Gx} \le 0.

    using the OSQP quadratic programming solver.

    :param kernel: :class:`~coreax.kernel.Kernel` instance implementing a kernel
        function :math:`k: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}`
    """

    kernel: Kernel

    def solve(
        self,
        x: Union[ArrayLike, Data],
        y: Union[ArrayLike, Data],
        epsilon: float = 1e-10,
        *,
        block_size: Union[int, None, tuple[Union[int, None], Union[int, None]]] = None,
        unroll: Union[int, bool, tuple[Union[int, bool], Union[int, bool]]] = 1,
        **solver_kwargs,
    ) -> Array:
        r"""
        Compute optimal weights given the simplex constraint.

        :param x: :class:`~coreax.data.Data` instance consisting of a :math:`n \times d`
            data array
        :param y: :class:`~coreax.data.Data` instance consisting of a :math:`m \times d`
            data array, representing a coreset
        :param epsilon: Small positive value to add to the kernel Gram matrix to aid
            numerical solver computations
        :param block_size: Block size passed to the ``self.kernel.compute_mean``
        :param unroll: Unroll parameter passed to ``self.kernel.compute_mean``
        :param solver_kwargs: Additional kwargs passed to ``jnp.linalg.solve``
        :return: Optimal weighting of points in ``y`` to represent ``x``
        """
        kernel_yx, kernel_yy = _prepare_kernel_system(
            self.kernel,
            as_data(x),
            as_data(y),
            epsilon,
            block_size=block_size,
            unroll=unroll,
        )
        return solve_qp(kernel_yy, kernel_yx, **solver_kwargs)


class JSBQWeightsOptimiser(WeightsOptimiser[_SupervisedData]):
    r"""
    Define the Joint Sequential Bayesian Quadrature (JSBQ) optimiser.

    Related references for this technique can be found in :cite:`huszar2016optimally`.
    Weights determined by JSBQ are equivalent to the unconstrained weighted joint
    maximum mean discrepancy (JMMD) optimum.

    The Bayesian quadrature estimate of the joint integral

    .. math::

        \int\int f(x, y) p(x, y) dx dy

    can be viewed as a  weighted version of joint kernel herding. The Bayesian
    quadrature weights, :math:`w_{BQ}`, are given by

    .. math::

        w_{BQ}^{(n)} = \sum_m z_m^T K_{mn}^{-1}

    for a supervised dataset :math:`(x, y)` with :math:`n` points, and coreset
    :math:`(\tilde{x}, \tilde{y})` of :math:`m` points. Here, for given tensor-product
    kernel :math:`k`, we have
    :math:`z = \int k((x, y), (\tilde{x}, \tilde{y}))p(x, y) dx dy`
    and :math:`K = k((\tilde{x}, \tilde{y}), (\tilde{x}, \tilde{y}))` in the above
    expression. See equation 20 in :cite:`huszar2016optimally` for further detail.

    :param feature_kernel: :class:`~coreax.kernel.Kernel` instance implementing a kernel
        function :math:`k: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}` on
        the feature space
    :param response_kernel: :class:`~coreax.kernel.Kernel` instance implementing a
        kernel function :math:`k: \mathbb{R}^p \times \mathbb{R}^p \rightarrow
        \mathbb{R}` on the response space
    """

    feature_kernel: Kernel
    response_kernel: Kernel

    def __init__(self, feature_kernel: Kernel, response_kernel: Kernel):
        """Initialise JSBQWeightsOptimiser class and build tensor-product kernel."""
        self.kernel = TensorProductKernel(
            feature_kernel=feature_kernel,
            response_kernel=response_kernel,
        )

    def solve(
        self,
        x: Union[tuple[ArrayLike, ArrayLike], SupervisedData],
        y: Union[tuple[ArrayLike, ArrayLike], SupervisedData],
        epsilon: float = 1e-10,
        *,
        block_size: Union[int, None, tuple[Union[int, None], Union[int, None]]] = None,
        unroll: Union[int, bool, tuple[Union[int, bool], Union[int, bool]]] = 1,
        **solver_kwargs,
    ) -> Array:
        r"""
        Calculate weights from Joint Sequential Bayesian Quadrature (JSBQ).

        References for this technique can be found in
        :cite:`huszar2016optimally`. These are equivalent to the unconstrained
        weighted joint maximum mean discrepancy (JMMD) optimum.

        Note that weights determined through SBQ do not need to sum to 1, and can be
        negative.

        :param x: :class:`~coreax.data.SupervisedData` instance consisting of
            :math:`n \times d` data array paired with :math:`n \times p` supervision
            array
        :param y::class:`~coreax.data.SupervisedData` instance consisting of
            :math:`m \times d` data array paired with :math:`m \times p` supervision
            array, representing a coreset
        :param epsilon: Small positive value to add to the kernel Gram matrix to aid
            numerical solver computations
        :param block_size: Block size passed to the ``self.kernel.compute_mean``
        :param unroll: Unroll parameter passed to ``self.kernel.compute_mean``
        :param solver_kwargs: Additional kwargs passed to ``jnp.linalg.solve``
        :return: Optimal weighting of points in ``y`` to represent ``x``
        """
        kernel_yx, kernel_yy = _prepare_kernel_system(
            self.kernel,
            as_supervised_data(x),
            as_supervised_data(y),
            epsilon,
            block_size=block_size,
            unroll=unroll,
        )
        return jnp.linalg.solve(kernel_yy, kernel_yx, **solver_kwargs)


class JMMDWeightsOptimiser(WeightsOptimiser[_SupervisedData]):
    r"""
    Define the JMMD weights optimiser class.

    This optimiser solves a simplex weight problem of the form:

    .. math::

        \mathbf{w}^{\mathrm{T}} \mathbf{k} \mathbf{w} +
        \bar{\mathbf{k}}^{\mathrm{T}} \mathbf{w} = 0

    subject to

    .. math::

        \mathbf{Aw} = \mathbf{1}, \qquad \mathbf{Gx} \le 0.

    using the OSQP quadratic programming solver.

    :param feature_kernel: :class:`~coreax.kernel.Kernel` instance implementing a kernel
        function :math:`k: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}` on
        the feature space
    :param response_kernel: :class:`~coreax.kernel.Kernel` instance implementing a
        kernel function :math:`k: \mathbb{R}^p \times \mathbb{R}^p \rightarrow
        \mathbb{R}` on the response space
    """

    feature_kernel: Kernel
    response_kernel: Kernel

    def __init__(self, feature_kernel: Kernel, response_kernel: Kernel):
        """Initialise JMMDWeightsOptimiser class and build tensor-product kernel."""
        self.kernel = TensorProductKernel(
            feature_kernel=feature_kernel,
            response_kernel=response_kernel,
        )

    def solve(
        self,
        x: Union[tuple[ArrayLike, ArrayLike], SupervisedData],
        y: Union[tuple[ArrayLike, ArrayLike], SupervisedData],
        epsilon: float = 1e-10,
        *,
        block_size: Union[int, None, tuple[Union[int, None], Union[int, None]]] = None,
        unroll: Union[int, bool, tuple[Union[int, bool], Union[int, bool]]] = 1,
        **solver_kwargs,
    ) -> Array:
        r"""
        Compute optimal weights given the simplex constraint.

        :param x: :class:`~coreax.data.SupervisedData` instance consisting of
            :math:`n \times d` data array paired with :math:`n \times p` supervision
            array
        :param y::class:`~coreax.data.SupervisedData` instance consisting of
            :math:`m \times d` data array paired with :math:`m \times p` supervision
            array, representing a coreset
        :param epsilon: Small positive value to add to the kernel Gram matrix to aid
            numerical solver computations
        :param block_size: Block size passed to the ``self.kernel.compute_mean``
        :param unroll: Unroll parameter passed to ``self.kernel.compute_mean``
        :param solver_kwargs: Additional kwargs passed to ``jnp.linalg.solve``
        :return: Optimal weighting of points in ``y`` to represent ``x``
        """
        kernel_yx, kernel_yy = _prepare_kernel_system(
            self.kernel,
            as_data(x),
            as_data(y),
            epsilon,
            block_size=block_size,
            unroll=unroll,
        )
        return solve_qp(kernel_yy, kernel_yx, **solver_kwargs)


@deprecated("Renamed to SBQWeightsOptimiser; will be removed in version 0.3.0")
class SBQ(SBQWeightsOptimiser):
    """
    Deprecated reference to :class:`~coreax.weights.SBQWeightsOptimiser`.

    Will be removed in version 0.3.0
    """


@deprecated("Renamed to `MMDWeightsOptimiser`; will be removed in version 0.3.0")
class MMD(MMDWeightsOptimiser):
    """
    Deprecated reference to :class:`~coreax.weights.MMDWeightsOptimiser`.

    Will be removed in version 0.3.0
    """
